{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLab Getting Started in Scala\n",
    "\n",
    "The Bigstep DataLab is a open data exploration service that offers data science, analytics and technology experimentation, built on our SparkArray, DataLake and on our highly flexible and high performance bare-metal infrastructure.\n",
    "\n",
    "This tutorial assumes some programming experience.\n",
    "\n",
    "## Uploading Data\n",
    "\n",
    "A private datalake (HDFS service) is used to store the data that the SparkArray uses. To upload data to a Bigstep Datalake, one would typically:\n",
    "1. upload data to the home directory of the datalake using commands like \"-put\"\n",
    "2. execute commands like \"-ls\" to ensure data was uploaded in the datalake\n",
    "\n",
    "```\n",
    "dl -ls /\n",
    "16/09/26 17:18:47 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "Found 3 items\n",
    "drwxrwxrwx   - hdfs supergroup          0 2018-09-15 13:12 /data_lake/dl1234/baseball\n",
    "drwxrwxrwt   - hdfs supergroup          0 2018-09-15 12:08 /data_lake/dl1234/tmp\n",
    "drwxr-xr-x   - hdfs supergroup          0 2018-09-15 12:08 /data_lake/dl1234/tmp/user\n",
    "```\n",
    "You can also execute the same commands on the master container!\n",
    "\n",
    "Data can be uploaded to the DataLake also by using the File Browser that is available in the DataLake File Browser tab in our user interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/* Allow the use of shell operations */\n",
    "import sys.process._\n",
    "\n",
    "/* Download data locally from the internet */\n",
    "\"wget http://www.exploredata.net/ftp/MLB2008.csv\".!\n",
    "\n",
    "/* Copy the downloaded file to Bigstep DataLake, using the relative path of the user's DataLake home directory */\n",
    "\"dl -put MLB2008.csv /\" !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Spark Context\n",
    "\n",
    "For all Spark functions to be available, a Spark context is initialized by default in the current notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/* import a SparkSession */\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "/* The SparkSession is conencted to the current Spark Master. Retrieve the current SparkSession using: */\n",
    "spark\n",
    "\n",
    "/* Retrieve the current SparkContext using: */\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDDs\n",
    "\n",
    "An Resilient Distributed Dataset is an array that is spread across multiple servers. It allows the programmer to abstract away the complexity of transforming large volumes of distributed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"wget http://seanlahman.com/files/database/baseballdatabank-master_2016-03-02.zip\".!\n",
    "\n",
    "\"apt-get install -y unzip\".!\n",
    "\"unzip baseballdatabank-master_2016-03-02.zip\".!\n",
    "\"rm -rf baseballdatabank-master_2016-03-02.zip\".!\n",
    "\n",
    "\"dl -put baseballdatabank-master/core/AllstarFull.csv /\".!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val textFile = sc.textFile(\"/AllstarFull.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textFile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textFile.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val linesWithRuth = textFile.filter( line => line.contains(\"ruth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linesWithRuth.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linesWithRuth.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linesWithRuth.saveAsTextFile(\"/lines_with_ruth-file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrames and SparkSQL\n",
    "\n",
    "A SparkDataFrame can also be registered as a temporary view in Spark SQL and that allows you to run SQL queries over its data. The sql function enables applications to run SQL queries programmatically and returns the result as a SparkDataFrame.\n",
    "\n",
    "Spark 2.3.0 has a built-in CSV reader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"dl -chmod 740 /AllstarFull.csv\" !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val allstar = spark.read.option(\"header\", \"true\").csv(\"/AllstarFull.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allstar.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allstar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allstar.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/* register this table as a \"table\" within the sql context. */\n",
    "allstar.createOrReplaceTempView(\"allstar\")\n",
    "\n",
    "/* SQL can be run over DataFrames that have been registered as a table. */\n",
    "val player = spark.sql(\"SELECT * FROM allstar WHERE playerID like '%ruth%' and yearID<1935\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ParquetFiles\n",
    "\n",
    "Parquet files are typically much faster and take up less space than CSVs and the DataLake supports them as well. As Spark is a clustering system the parquet files are composed out of many fragments generated by the workres independently. The collection of files is operated as a single big table by SparkSQL.\n",
    "\n",
    "To write the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.util.Calendar\n",
    "\n",
    "val now = Calendar.getInstance().getTimeInMillis()\n",
    "\n",
    "val path=\"/allstar-\"+now.toString()+\".parquet\"\n",
    "player.write.format(\"parquet\").save(path)\n",
    "\n",
    "\"dl -ls /\".!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dfParquet = spark.read.parquet(path)\n",
    "dfParquet.createOrReplaceTempView(\"player\")\n",
    "spark.sql(\"SELECT playerID,YearID FROM player\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources \n",
    "\n",
    "[Apache Spark 2.3.0 Programming Guide](http://spark.apache.org/docs/latest/sql-programming-guide.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
