{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLab Getting Started in R\n",
    "\n",
    "The Bigstep DataLab is a open data exploration service that offers data science, analytics and technology experimentation, built on our SparkArray, DataLake and on our highly flexible and high performance bare-metal infrastructure.\n",
    "\n",
    "This tutorial assumes some programming experience.\n",
    "\n",
    "## Uploading Data\n",
    "\n",
    "A private datalake (HDFS service) is used to store the data that the SparkArray uses. To upload data to a Bigstep Datalake, one would typically:\n",
    "1. upload data to the home directory of the datalake using commands like \"-put\"\n",
    "2. execute commands like \"-ls\" to ensure data was uploaded in the datalake\n",
    "\n",
    "```\n",
    "dl -ls /\n",
    "16/09/26 17:18:47 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "Found 3 items\n",
    "drwxrwxrwx   - hdfs supergroup          0 2018-09-15 13:12 /data_lake/dl1234/baseball\n",
    "drwxrwxrwt   - hdfs supergroup          0 2018-09-15 12:08 /data_lake/dl1234/tmp\n",
    "drwxr-xr-x   - hdfs supergroup          0 2018-09-15 12:08 /data_lake/dl1234/tmp/user\n",
    "```\n",
    "You can also execute the same commands on the master container!\n",
    "\n",
    "Data can be uploaded to the DataLake also by using the File Browser that is available in the DataLake File Browser tab in our user interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow the use of shell operations \n",
    "system(\"wget http://www.exploredata.net/ftp/MLB2008.csv\", intern=TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the downloaded file to Bigstep DataLake, using the path specified under the Spark tab in the Bigstep Control Center\n",
    "system(\"dl -put MLB2008.csv /\", intern=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Spark Context\n",
    "\n",
    "For all Spark functions to be available, a Spark context has to be initialized in the current notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(SparkR)\n",
    "sparkR.session(appName = \"R\", sparkConfig = list(spark.warehouse.dir=\"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDDs\n",
    "\n",
    "An Resilient Distributed Dataset is an array that is spread across multiple servers. It allows the programmer to abstract away the complexity of transforming large volumes of distributed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system(\"wget http://seanlahman.com/files/database/baseballdatabank-master_2016-03-02.zip\", intern=TRUE)\n",
    "\n",
    "system(\"apt-get install -y unzip\", intern=TRUE)\n",
    "system(\"unzip baseballdatabank-master_2016-03-02.zip\", intern=TRUE)\n",
    "system(\"rm -rf baseballdatabank-master_2016-03-02.zip\", intern=TRUE)\n",
    "\n",
    "system(\"dl -put baseballdatabank-master/core/AllstarFull.csv /\", intern=TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system(\"dl -chmod 777 /tmp/hive\", intern=TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sys.getenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc <- sparkR.session()\n",
    " \n",
    "people <- read.df(\"/AllstarFull.csv\", \"csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count(people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first(people)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames and SparkRSQL\n",
    "\n",
    "A SparkDataFrame can also be registered as a temporary view in Spark SQL and that allows you to run SQL queries over its data. The sql function enables applications to run SQL queries programmatically and returns the result as a SparkDataFrame.\n",
    "\n",
    "Spark 2.3.0. has a built-in CSV reader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a json file\n",
    "dfPeople <- read.df(\"file:///opt/spark-2.3.0-bin-hadoop2.7/examples/src/main/resources/people.json\", \"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame as a SQL temporary view\n",
    "createOrReplaceTempView(dfPeople, \"people\")\n",
    "\n",
    "# SQL statements can be run by using the sql method\n",
    "teenagers <- sql(\"SELECT name FROM people WHERE age >= 13 AND age <= 19\")\n",
    "head(teenagers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
