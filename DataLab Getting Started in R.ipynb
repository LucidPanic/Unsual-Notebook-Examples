{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLab Getting Started in R\n",
    "\n",
    "The Bigstep DataLab is a open data exploration service that offers data science, analytics and technology experimentation, built on our SparkArray, DataLake and on our highly flexible and high performance bare-metal infrastructure.\n",
    "\n",
    "This tutorial assumes some programming experience.\n",
    "\n",
    "## Uploading Data\n",
    "\n",
    "A private datalake (HDFS service) is used to store the data that the SparkArray uses. To upload data to an HDFS cluster one would typically:\n",
    "1. download the hadoop binaries (2.7.x) from a mirror like [here](http://apache.claz.org/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz)  - rather large  (240mb)\n",
    "2. unarchive \n",
    "3. execute commands like \"-ls\".\n",
    "\n",
    "```\n",
    "/hadoop-2.7.3/bin/hdfs dfs -ls hdfs://headnodes-8885.cluster-8885.us-private-datalake.7.bigstep.io/\n",
    "16/09/26 17:18:47 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "Found 3 items\n",
    "drwxrwxrwx   - hdfs supergroup          0 2016-09-22 13:12 hdfs://headnodes-8885.cluster-8885.us-private-datalake.7.bigstep.io/baseball\n",
    "drwxrwxrwt   - hdfs supergroup          0 2016-09-22 12:08 hdfs://headnodes-8885.cluster-8885.us-private-datalake.7.bigstep.io/tmp\n",
    "drwxr-xr-x   - hdfs supergroup          0 2016-09-22 12:08 hdfs://headnodes-8885.cluster-8885.us-private-datalake.7.bigstep.io/user\n",
    "```\n",
    "\n",
    "\n",
    "You can also execute the same commands on the master container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Allow the use of shell operations \n",
    "system(\"wget http://www.exploredata.net/ftp/MLB2008.csv\", intern=TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Copy the downloaded file to Bigstep DataLake, using the path specified under the Spark tab in the Bigstep Control Center\n",
    "system(\"hdfs dfs -put MLB2008.csv /\", intern=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Spark Context\n",
    "\n",
    "For all Spark functions to be available, a Spark context has to be initialized in the current notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "library(SparkR)\n",
    "sparkR.session(appName = \"R\", sparkConfig = list(spark.warehouse.dir=\"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDDs\n",
    "\n",
    "An Resilient Distributed Dataset is an array that is spread across multiple servers. It allows the programmer to abstract away the complexity of transforming large volumes of distributed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "system(\"wget http://seanlahman.com/files/database/baseballdatabank-master_2016-03-02.zip\", intern=TRUE)\n",
    "\n",
    "system(\"apk add unzip\", intern=TRUE)\n",
    "system(\"unzip baseballdatabank-master_2016-03-02.zip\", intern=TRUE)\n",
    "system(\"rm -rf baseballdatabank-master_2016-03-02.zip\", intern=TRUE)\n",
    "\n",
    "system(\"hdfs dfs -put baseballdatabank-master/core/AllstarFull.csv /\", intern=TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "system(\"hdfs dfs -chmod 777 /tmp/hive\", intern=TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Sys.getenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc <- sparkR.session()\n",
    " \n",
    "people <- read.df(\"/AllstarFull.csv\", \"csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count(people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first(people)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames and SparkRSQL\n",
    "\n",
    "A SparkDataFrame can also be registered as a temporary view in Spark SQL and that allows you to run SQL queries over its data. The sql function enables applications to run SQL queries programmatically and returns the result as a SparkDataFrame.\n",
    "\n",
    "Spark 2.0.0. has a built-in CSV reader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read a json file\n",
    "dfPeople <- read.df(\"file:///opt/spark-2.1.0-bin-hadoop2.7/examples/src/main/resources/people.json\", \"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Register the DataFrame as a SQL temporary view\n",
    "createOrReplaceTempView(dfPeople, \"people\")\n",
    "\n",
    "# SQL statements can be run by using the sql method\n",
    "teenagers <- sql(\"SELECT name FROM people WHERE age >= 13 AND age <= 19\")\n",
    "head(teenagers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
