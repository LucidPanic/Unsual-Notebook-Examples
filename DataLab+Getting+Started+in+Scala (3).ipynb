{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLab Getting Started in Scala\n",
    "\n",
    "The Bigstep DataLab is a open data exploration service that offers data science, analytics and technology experimentation, built on our SparkArray, DataLake and on our highly flexible and high performance bare-metal infrastructure.\n",
    "\n",
    "This tutorial assumes some programming experience.\n",
    "\n",
    "## Uploading Data\n",
    "\n",
    "A private datalake (HDFS service) is used to store the data that the SparkArray uses. To upload data to an HDFS cluster one would typically:\n",
    "1. download the hadoop binaries (2.7.x) from a mirror like [here](http://apache.claz.org/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz)  - rather large  (240mb)\n",
    "2. unarchive \n",
    "3. execute commands like \"-ls\".\n",
    "\n",
    "```\n",
    "/hadoop-2.7.3/bin/hdfs dfs -ls hdfs://headnodes-8885.cluster-8885.us-private-datalake.7.bigstep.io/\n",
    "16/09/26 17:18:47 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "Found 3 items\n",
    "drwxrwxrwx   - hdfs supergroup          0 2016-09-22 13:12 hdfs://headnodes-8885.cluster-8885.us-private-datalake.7.bigstep.io/baseball\n",
    "drwxrwxrwt   - hdfs supergroup          0 2016-09-22 12:08 hdfs://headnodes-8885.cluster-8885.us-private-datalake.7.bigstep.io/tmp\n",
    "drwxr-xr-x   - hdfs supergroup          0 2016-09-22 12:08 hdfs://headnodes-8885.cluster-8885.us-private-datalake.7.bigstep.io/user\n",
    "```\n",
    "\n",
    "\n",
    "You can also execute the same commands on the master container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "/* import a SparkSession */\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "/* Create a SparkSession that is conencted to the current Spark Master. Retrieve the SparkMaster URL from the\n",
    "   Spark tab in the Bigstep Control Center and replace the value in the line below: */\n",
    "val sparkSession = SparkSession.builder.master(\"spark://container-cluster-2367:7077\").appName(\"my-spark-app\").getOrCreate()\n",
    "\n",
    "/* Allow the use of shell operations */\n",
    "import sys.process._\n",
    "\n",
    "\"wget http://www.exploredata.net/ftp/MLB2008.csv\" !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "/* Copy the downloaded file to Bigstep DataLake, using the relative path of the user's DataLake home directory */\n",
    "\"hdfs dfs -put MLB2008.csv /\" !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Spark Context\n",
    "\n",
    "For all Spark functions to be available, a Spark context is initialized by default in the current notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDDs\n",
    "\n",
    "An Resilient Distributed Dataset is an array that is spread across multiple servers. It allows the programmer to abstract away the complexity of transforming large volumes of distributed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"wget http://seanlahman.com/files/database/baseballdatabank-master_2016-03-02.zip\" !\n",
    "\n",
    "\"apt-get install -y unzip\" !\n",
    "\"unzip baseballdatabank-master_2016-03-02.zip\" !\n",
    "\"rm -rf baseballdatabank-master_2016-03-02.zip\" !\n",
    "\n",
    "\"hdfs dfs -put baseballdatabank-master/core/AllstarFull.csv /\" !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val textFile = sc.textFile(\"/AllstarFull.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textFile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textFile.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val linesWithRuth = textFile.filter( line => line.contains(\"ruth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linesWithRuth.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linesWithRuth.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linesWithRuth.saveAsTextFile(\"/lines_with_ruth-file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrames and SparkSQL\n",
    "\n",
    "A SparkDataFrame can also be registered as a temporary view in Spark SQL and that allows you to run SQL queries over its data. The sql function enables applications to run SQL queries programmatically and returns the result as a SparkDataFrame.\n",
    "\n",
    "Spark 2.1.0 has a built-in CSV reader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"hdfs dfs -chmod 740 /AllstarFull.csv\" !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val allstar = sparkSession.read.option(\"header\", \"true\").csv(\"/AllstarFull.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allstar.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allstar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allstar.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "/* register this table as a \"table\" within the sql context. */\n",
    "allstar.createOrReplaceTempView(\"allstar\")\n",
    "\n",
    "/* SQL can be run over DataFrames that have been registered as a table. */\n",
    "val player = spark.sql(\"SELECT * FROM allstar WHERE playerID like '%ruth%' and yearID<1935\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "player.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ParquetFiles\n",
    "\n",
    "Parquet files are typically much faster and take up less space than CSVs and the DataLake supports them as well. As Spark is a clustering system the parquet files are composed out of many fragments generated by the workres independently. The collection of files is operated as a single big table by SparkSQL.\n",
    "\n",
    "To write the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import java.util.Calendar\n",
    "\n",
    "val now = Calendar.getInstance().getTimeInMillis()\n",
    "\n",
    "val path=\"/allstar-\"+now.toString()+\".parquet\"\n",
    "player.write.save(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val dfParquet = spark.read.parquet(path)\n",
    "dfParquet.createOrReplaceTempView(\"player\")\n",
    "spark.sql(\"SELECT playerID,YearID FROM player\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources \n",
    "\n",
    "[Apache Spark 2.1.0 Programming Guide](http://spark.apache.org/docs/latest/sql-programming-guide.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
